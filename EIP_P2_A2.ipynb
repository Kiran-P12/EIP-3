{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP_P2_A2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "bVsnH7ZAXtKX",
        "ZruNlxLJX1Us",
        "4ord4wQ4X7Dh",
        "CZgeLDXQYB3k",
        "8JAJf5QqfDca",
        "8Xx44DXmYK2I",
        "0caxY_mNYNpI",
        "PG5L2P5RYRsZ",
        "nn_3ZI20YV_-"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVsnH7ZAXtKX",
        "colab_type": "text"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDXE7pbO14lS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Larger LSTM network and generate textimport sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Furw5quR59Ge",
        "colab_type": "code",
        "outputId": "0f1dd7a6-65de-456c-ee07-64a50d4cc49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUPAkXY7e6T",
        "colab_type": "code",
        "outputId": "69578e1e-22a9-4499-d2c0-4bef96004fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        }
      },
      "source": [
        "!ls gdrive/\"My Drive\"/\"EIP 3\"/\"phase 2\"/\"A2\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights-improvement-01-1.5494-bigger.hdf5\n",
            "weights-improvement-01-1.6597-bigger.hdf5\n",
            "weights-improvement-01-2.7005-bigger.hdf5\n",
            "weights-improvement-01-2.8524-bigger.hdf5\n",
            "weights-improvement-02-1.5372-bigger.hdf5\n",
            "weights-improvement-02-1.6467-bigger.hdf5\n",
            "weights-improvement-02-2.5432-bigger.hdf5\n",
            "weights-improvement-03-1.5267-bigger.hdf5\n",
            "weights-improvement-03-1.6373-bigger.hdf5\n",
            "weights-improvement-03-2.4100-bigger.hdf5\n",
            "weights-improvement-04-1.5219-bigger.hdf5\n",
            "weights-improvement-04-1.6250-bigger.hdf5\n",
            "weights-improvement-04-2.2997-bigger.hdf5\n",
            "weights-improvement-05-1.5091-bigger.hdf5\n",
            "weights-improvement-05-1.6141-bigger.hdf5\n",
            "weights-improvement-05-2.2144-bigger.hdf5\n",
            "weights-improvement-06-1.5070-bigger.hdf5\n",
            "weights-improvement-06-1.6006-bigger.hdf5\n",
            "weights-improvement-06-2.1451-bigger.hdf5\n",
            "weights-improvement-07-1.4891-bigger.hdf5\n",
            "weights-improvement-07-1.5858-bigger.hdf5\n",
            "weights-improvement-07-2.0896-bigger.hdf5\n",
            "weights-improvement-08-1.4846-bigger.hdf5\n",
            "weights-improvement-08-1.5773-bigger.hdf5\n",
            "weights-improvement-08-2.0437-bigger.hdf5\n",
            "weights-improvement-09-1.4788-bigger.hdf5\n",
            "weights-improvement-09-1.5694-bigger.hdf5\n",
            "weights-improvement-09-2.0013-bigger.hdf5\n",
            "weights-improvement-10-1.4701-bigger.hdf5\n",
            "weights-improvement-10-1.5558-bigger.hdf5\n",
            "weights-improvement-10-1.9683-bigger.hdf5\n",
            "weights-improvement-11-1.4626-bigger.hdf5\n",
            "weights-improvement-11-1.9350-bigger.hdf5\n",
            "weights-improvement-12-1.4531-bigger.hdf5\n",
            "weights-improvement-12-1.9104-bigger.hdf5\n",
            "weights-improvement-13-1.4504-bigger.hdf5\n",
            "weights-improvement-13-1.8810-bigger.hdf5\n",
            "weights-improvement-14-1.4385-bigger.hdf5\n",
            "weights-improvement-14-1.8584-bigger.hdf5\n",
            "weights-improvement-15-1.8293-bigger.hdf5\n",
            "weights-improvement-16-1.8111-bigger.hdf5\n",
            "weights-improvement-17-1.7902-bigger.hdf5\n",
            "weights-improvement-18-1.7718-bigger.hdf5\n",
            "weights-improvement-19-1.7469-bigger.hdf5\n",
            "weights-improvement-20-1.7335-bigger.hdf5\n",
            "weights-improvement-21-1.7212-bigger.hdf5\n",
            "weights-improvement-22-1.7098-bigger.hdf5\n",
            "weights-improvement-23-1.6886-bigger.hdf5\n",
            "weights-improvement-24-1.6732-bigger.hdf5\n",
            "wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udkHwKaX7gyr",
        "colab_type": "code",
        "outputId": "b336c188-16f0-4f1d-eb52-1326d56fea84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "!wc -l gdrive/\"My Drive\"/\"EIP 3\"/\"phase 2\"/\"A2\"/wonderland.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3330 gdrive/My Drive/EIP 3/phase 2/A2/wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_EiPXUS7mEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"gdrive/My Drive/EIP 3/phase 2/A2/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgZ01epGBude",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq037RdsF65z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars.remove('!')\n",
        "chars.remove('\"')\n",
        "chars.remove('(')\n",
        "chars.remove(')')\n",
        "chars.remove('*')\n",
        "chars.remove(',')\n",
        "chars.remove('-')\n",
        "chars.remove(':')\n",
        "chars.remove(';')\n",
        "chars.remove('?')\n",
        "chars.remove('[')\n",
        "chars.remove(']')\n",
        "chars.remove('_')\n",
        "chars.remove('\\'')\n",
        "chars.remove('\\r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jdxaTC4F_s7",
        "colab_type": "code",
        "outputId": "ad383e0a-0094-479d-d700-c23b3522ea04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "chars"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '.',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbtOlIO5LXOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text = raw_text.replace(\"!\", \"\")\n",
        "raw_text = raw_text.replace(\"\\\"\", \"\")\n",
        "raw_text = raw_text.replace(\"(\", \"\")\n",
        "raw_text = raw_text.replace(\")\", \"\")\n",
        "raw_text = raw_text.replace(\"*\", \"\")\n",
        "raw_text = raw_text.replace(\",\", \"\")\n",
        "raw_text = raw_text.replace(\"-\", \"\")\n",
        "raw_text = raw_text.replace(\":\", \"\")\n",
        "raw_text = raw_text.replace(\";\", \"\")\n",
        "raw_text = raw_text.replace(\"?\", \"\")\n",
        "raw_text = raw_text.replace(\"[\", \"\")\n",
        "raw_text = raw_text.replace(\"]\", \"\")\n",
        "raw_text = raw_text.replace(\"_\", \"\")\n",
        "raw_text = raw_text.replace(\"\\r\", \"\")\n",
        "raw_text = raw_text.replace(\"'\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjG9Pk46MLFN",
        "colab_type": "code",
        "outputId": "40705094-4b53-46ca-ac83-4e939594dd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "raw_text[:1000]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chapter i. down the rabbithole\\n\\nalice was beginning to get very tired of sitting by her sister on the\\nbank and of having nothing to do once or twice she had peeped into the\\nbook her sister was reading but it had no pictures or conversations in\\nit and what is the use of a book thought alice without pictures or\\nconversations\\n\\nso she was considering in her own mind as well as she could for the\\nhot day made her feel very sleepy and stupid whether the pleasure\\nof making a daisychain would be worth the trouble of getting up and\\npicking the daisies when suddenly a white rabbit with pink eyes ran\\nclose by her.\\n\\nthere was nothing so very remarkable in that nor did alice think it so\\nvery much out of the way to hear the rabbit say to itself oh dear\\noh dear i shall be late when she thought it over afterwards it\\noccurred to her that she ought to have wondered at this but at the time\\nit all seemed quite natural but when the rabbit actually took a watch\\nout of its waistcoatpocket and looked at it and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTTOxdI0GlPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8zImbVH5bN",
        "colab_type": "code",
        "outputId": "ada79364-cbb1-4ef2-928c-2715e39b1647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Total Characters: ', 137014)\n",
            "('Total Vocab: ', 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOk3IddDOiLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = raw_text.split('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ6MyUMXVOWo",
        "colab_type": "code",
        "outputId": "7f89c7b8-c512-4124-b7da-c2af5ac7c2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "splits[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chapter i',\n",
              " ' down the rabbithole\\n\\nalice was beginning to get very tired of sitting by her sister on the\\nbank and of having nothing to do once or twice she had peeped into the\\nbook her sister was reading but it had no pictures or conversations in\\nit and what is the use of a book thought alice without pictures or\\nconversations\\n\\nso she was considering in her own mind as well as she could for the\\nhot day made her feel very sleepy and stupid whether the pleasure\\nof making a daisychain would be worth the trouble of getting up and\\npicking the daisies when suddenly a white rabbit with pink eyes ran\\nclose by her',\n",
              " '\\n\\nthere was nothing so very remarkable in that nor did alice think it so\\nvery much out of the way to hear the rabbit say to itself oh dear\\noh dear i shall be late when she thought it over afterwards it\\noccurred to her that she ought to have wondered at this but at the time\\nit all seemed quite natural but when the rabbit actually took a watch\\nout of its waistcoatpocket and looked at it and then hurried on\\nalice started to her feet for it flashed across her mind that she had\\nnever before seen a rabbit with either a waistcoatpocket or a watch\\nto take out of it and burning with curiosity she ran across the field\\nafter it and fortunately was just in time to see it pop down a large\\nrabbithole under the hedge',\n",
              " '\\n\\nin another moment down went alice after it never once considering how\\nin the world she was to get out again',\n",
              " '\\n\\nthe rabbithole went straight on like a tunnel for some way and then\\ndipped suddenly down so suddenly that alice had not a moment to think\\nabout stopping herself before she found herself falling down a very deep\\nwell',\n",
              " '\\n\\neither the well was very deep or she fell very slowly for she had\\nplenty of time as she went down to look about her and to wonder what was\\ngoing to happen next',\n",
              " ' first she tried to look down and make out what\\nshe was coming to but it was too dark to see anything then she\\nlooked at the sides of the well and noticed that they were filled with\\ncupboards and bookshelves here and there she saw maps and pictures\\nhung upon pegs',\n",
              " ' she took down a jar from one of the shelves as\\nshe passed it was labelled orange marmalade but to her great\\ndisappointment it was empty she did not like to drop the jar for fear\\nof killing somebody so managed to put it into one of the cupboards as\\nshe fell past it',\n",
              " '\\n\\nwell thought alice to herself after such a fall as this i shall\\nthink nothing of tumbling down stairs how brave theyll all think me at\\nhome why i wouldnt say anything about it even if i fell off the top\\nof the house which was very likely true',\n",
              " '\\n\\ndown down down']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIZAoOiEeDwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch_item(string, seq_length):\n",
        "  dataX = []\n",
        "  dataY = []\n",
        "  if len(string)>seq_length :\n",
        "    for i in range(0, len(string) - seq_length, 1):\n",
        "      seq_in = string[i:i + seq_length]\n",
        "      seq_out = string[i + seq_length]\n",
        "      dataX.append(seq_in)\n",
        "      dataY.append(seq_out)\n",
        "  #     dataX.append([char_to_int[char] for char in seq_in])\n",
        "  #     dataY.append(char_to_int[seq_out])\n",
        "  else:\n",
        "    dataX.append(string[:-1])\n",
        "    dataY.append(string[-1])\n",
        "  return dataX, dataY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvIep8dzeanC",
        "colab_type": "code",
        "outputId": "13166a53-94d9-488f-8775-472413976d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "get_batch_item(\"kiran.\", 3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['kir', 'ira', 'ran'], ['a', 'n', '.'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chFcl_hZfaI3",
        "colab_type": "code",
        "outputId": "b75ced6f-8ae9-4fc0-b830-35d8e038f476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "get_batch_item(\"kiran\", 6)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['kira'], ['n'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uZ5JD-LJ0YT",
        "colab_type": "code",
        "outputId": "298888c5-9fd1-42f8-e6cb-1d5e8ae0e804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 70\n",
        "dataX = []\n",
        "dataY = []\n",
        "for split in splits:\n",
        "  Xs, Ys = get_batch_item(split, seq_length)\n",
        "  for x in Xs:\n",
        "#       dataX.append(x)\n",
        "    dataX.append([char_to_int[char] for char in x])\n",
        "  for y in Ys:\n",
        "#       dataY.append(y)\n",
        "    dataY.append(char_to_int[y])\n",
        "n_patterns = len(dataX)\n",
        "y_patterns = len(dataY)\n",
        "print \"Total Patterns: \", n_patterns , y_patterns "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  76916 76916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WtotdTzm7DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "dataX = pad_sequences(dataX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Qm4-Dkbh3I",
        "colab_type": "code",
        "outputId": "ada0940e-d18d-44a3-8f0f-4d1b99e41f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "print([len(x) for x in dataX[:5]])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[70, 70, 70, 70, 70]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJEp6op0bhyq",
        "colab_type": "code",
        "outputId": "d01a36bb-d875-4c6d-ec68-50605ed16079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "[x for x in dataY[:5]]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11, 1, 4, 27, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZruNlxLJX1Us",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJJbZoDuH_nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"gdrive/My Drive/EIP 3/phase 2/A2/\"+\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn53I1otIJlN",
        "colab_type": "code",
        "outputId": "c8d65c3e-62ed-42c9-cabc-15642e0b94db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=1, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 17:24:35.237792 139889999755136 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0723 17:24:35.244981 139889999755136 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0723 17:24:35.258846 139889999755136 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0723 17:24:35.278593 139889999755136 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0723 17:24:36.149082 139889999755136 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0723 17:24:36.284706 139889999755136 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 2.8524\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.85243, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-01-2.8524-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3a65481950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TMprLof4YPI",
        "colab_type": "code",
        "outputId": "35ece518-5725-4e99-87e1-ac028df3656b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=24, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 2.7005\n",
            "\n",
            "Epoch 00001: loss improved from 2.85243 to 2.70050, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-01-2.7005-bigger.hdf5\n",
            "Epoch 2/24\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 2.5432\n",
            "\n",
            "Epoch 00002: loss improved from 2.70050 to 2.54322, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-02-2.5432-bigger.hdf5\n",
            "Epoch 3/24\n",
            "76916/76916 [==============================] - 166s 2ms/step - loss: 2.4100\n",
            "\n",
            "Epoch 00003: loss improved from 2.54322 to 2.41000, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-03-2.4100-bigger.hdf5\n",
            "Epoch 4/24\n",
            "76916/76916 [==============================] - 163s 2ms/step - loss: 2.2997\n",
            "\n",
            "Epoch 00004: loss improved from 2.41000 to 2.29973, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-04-2.2997-bigger.hdf5\n",
            "Epoch 5/24\n",
            "76916/76916 [==============================] - 162s 2ms/step - loss: 2.2144\n",
            "\n",
            "Epoch 00005: loss improved from 2.29973 to 2.21442, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-05-2.2144-bigger.hdf5\n",
            "Epoch 6/24\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 2.1451\n",
            "\n",
            "Epoch 00006: loss improved from 2.21442 to 2.14515, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-06-2.1451-bigger.hdf5\n",
            "Epoch 7/24\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 2.0896\n",
            "\n",
            "Epoch 00007: loss improved from 2.14515 to 2.08963, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-07-2.0896-bigger.hdf5\n",
            "Epoch 8/24\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 2.0437\n",
            "\n",
            "Epoch 00008: loss improved from 2.08963 to 2.04370, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-08-2.0437-bigger.hdf5\n",
            "Epoch 9/24\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 2.0013\n",
            "\n",
            "Epoch 00009: loss improved from 2.04370 to 2.00127, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-09-2.0013-bigger.hdf5\n",
            "Epoch 10/24\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 1.9683\n",
            "\n",
            "Epoch 00010: loss improved from 2.00127 to 1.96829, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-10-1.9683-bigger.hdf5\n",
            "Epoch 11/24\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 1.9350\n",
            "\n",
            "Epoch 00011: loss improved from 1.96829 to 1.93496, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-11-1.9350-bigger.hdf5\n",
            "Epoch 12/24\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 1.9104\n",
            "\n",
            "Epoch 00012: loss improved from 1.93496 to 1.91038, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-12-1.9104-bigger.hdf5\n",
            "Epoch 13/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.8810\n",
            "\n",
            "Epoch 00013: loss improved from 1.91038 to 1.88097, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-13-1.8810-bigger.hdf5\n",
            "Epoch 14/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.8584\n",
            "\n",
            "Epoch 00014: loss improved from 1.88097 to 1.85839, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-14-1.8584-bigger.hdf5\n",
            "Epoch 15/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.8293\n",
            "\n",
            "Epoch 00015: loss improved from 1.85839 to 1.82926, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-15-1.8293-bigger.hdf5\n",
            "Epoch 16/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.8111\n",
            "\n",
            "Epoch 00016: loss improved from 1.82926 to 1.81106, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-16-1.8111-bigger.hdf5\n",
            "Epoch 17/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.7902\n",
            "\n",
            "Epoch 00017: loss improved from 1.81106 to 1.79018, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-17-1.7902-bigger.hdf5\n",
            "Epoch 18/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.7718\n",
            "\n",
            "Epoch 00018: loss improved from 1.79018 to 1.77182, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-18-1.7718-bigger.hdf5\n",
            "Epoch 19/24\n",
            "76916/76916 [==============================] - 166s 2ms/step - loss: 1.7469\n",
            "\n",
            "Epoch 00019: loss improved from 1.77182 to 1.74687, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-19-1.7469-bigger.hdf5\n",
            "Epoch 20/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.7335\n",
            "\n",
            "Epoch 00020: loss improved from 1.74687 to 1.73348, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-20-1.7335-bigger.hdf5\n",
            "Epoch 21/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.7212\n",
            "\n",
            "Epoch 00021: loss improved from 1.73348 to 1.72124, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-21-1.7212-bigger.hdf5\n",
            "Epoch 22/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.7098\n",
            "\n",
            "Epoch 00022: loss improved from 1.72124 to 1.70980, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-22-1.7098-bigger.hdf5\n",
            "Epoch 23/24\n",
            "76916/76916 [==============================] - 167s 2ms/step - loss: 1.6886\n",
            "\n",
            "Epoch 00023: loss improved from 1.70980 to 1.68859, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-23-1.6886-bigger.hdf5\n",
            "Epoch 24/24\n",
            "76916/76916 [==============================] - 166s 2ms/step - loss: 1.6732\n",
            "\n",
            "Epoch 00024: loss improved from 1.68859 to 1.67320, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-24-1.6732-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3a505dd350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phQNU-wB1bPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-01-2.8524-bigger.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfjEUZcej187",
        "colab_type": "code",
        "outputId": "30958d75-df56-4901-8206-a96355cf4fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print \"Seed:\"\n",
        "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
        "# generate characters\n",
        "output=\"\"\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "#   print(x.shape)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  output = output+result\n",
        "  pattern = numpy.append(pattern , index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(str(output))\n",
        "print \"\\nDone.\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" all\n",
            "think nothing of tumbling down stairs how brave theyll all think m \"\n",
            "e gor ier iead to seadi oe the had been wotle sather anl ereatu and seadi oe the had noted ano anice anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anlce anl\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ord4wQ4X7Dh",
        "colab_type": "text"
      },
      "source": [
        "## Epochs 25-35"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW1XPVFAu_1c",
        "colab_type": "code",
        "outputId": "e4992c8c-687d-41ba-d48e-fc5b4cf1a847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=10, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.6597\n",
            "\n",
            "Epoch 00001: loss improved from 1.67320 to 1.65972, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-01-1.6597-bigger.hdf5\n",
            "Epoch 2/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.6467\n",
            "\n",
            "Epoch 00002: loss improved from 1.65972 to 1.64672, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-02-1.6467-bigger.hdf5\n",
            "Epoch 3/10\n",
            "76916/76916 [==============================] - 163s 2ms/step - loss: 1.6373\n",
            "\n",
            "Epoch 00003: loss improved from 1.64672 to 1.63730, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-03-1.6373-bigger.hdf5\n",
            "Epoch 4/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.6250\n",
            "\n",
            "Epoch 00004: loss improved from 1.63730 to 1.62503, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-04-1.6250-bigger.hdf5\n",
            "Epoch 5/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.6141\n",
            "\n",
            "Epoch 00005: loss improved from 1.62503 to 1.61408, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-05-1.6141-bigger.hdf5\n",
            "Epoch 6/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.6006\n",
            "\n",
            "Epoch 00006: loss improved from 1.61408 to 1.60057, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-06-1.6006-bigger.hdf5\n",
            "Epoch 7/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5858\n",
            "\n",
            "Epoch 00007: loss improved from 1.60057 to 1.58583, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-07-1.5858-bigger.hdf5\n",
            "Epoch 8/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5773\n",
            "\n",
            "Epoch 00008: loss improved from 1.58583 to 1.57731, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-08-1.5773-bigger.hdf5\n",
            "Epoch 9/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5694\n",
            "\n",
            "Epoch 00009: loss improved from 1.57731 to 1.56938, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-09-1.5694-bigger.hdf5\n",
            "Epoch 10/10\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5558\n",
            "\n",
            "Epoch 00010: loss improved from 1.56938 to 1.55584, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-10-1.5558-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3a505dd690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOkshy8SPTQR",
        "colab_type": "code",
        "outputId": "ad4d8176-148d-43c5-e767-6d88f37ec2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print \"Seed:\"\n",
        "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
        "# generate characters\n",
        "output=\"\"\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "#   print(x.shape)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  output = output+result\n",
        "  pattern = numpy.append(pattern , index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(str(output))\n",
        "print \"\\nDone.\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" e taught us said the mock turtle\n",
            "angrily really you are very dull\n",
            "\n",
            "you \"\n",
            " suop anice anlce as seallng tometny temember the hell hoooowed to eone oear the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge hardenert the whe marge h\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZgeLDXQYB3k",
        "colab_type": "text"
      },
      "source": [
        "## Epochs 35-50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auLuH3IbPTMd",
        "colab_type": "code",
        "outputId": "116bc84e-ceab-4c7e-9515-ec00e32ce242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=15, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 1.5494\n",
            "\n",
            "Epoch 00001: loss improved from 1.55584 to 1.54935, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-01-1.5494-bigger.hdf5\n",
            "Epoch 2/15\n",
            "76916/76916 [==============================] - 165s 2ms/step - loss: 1.5372\n",
            "\n",
            "Epoch 00002: loss improved from 1.54935 to 1.53725, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-02-1.5372-bigger.hdf5\n",
            "Epoch 3/15\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5267\n",
            "\n",
            "Epoch 00003: loss improved from 1.53725 to 1.52668, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-03-1.5267-bigger.hdf5\n",
            "Epoch 4/15\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5219\n",
            "\n",
            "Epoch 00004: loss improved from 1.52668 to 1.52192, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-04-1.5219-bigger.hdf5\n",
            "Epoch 5/15\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5091\n",
            "\n",
            "Epoch 00005: loss improved from 1.52192 to 1.50910, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-05-1.5091-bigger.hdf5\n",
            "Epoch 6/15\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.5070\n",
            "\n",
            "Epoch 00006: loss improved from 1.50910 to 1.50700, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-06-1.5070-bigger.hdf5\n",
            "Epoch 7/15\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.4891\n",
            "\n",
            "Epoch 00007: loss improved from 1.50700 to 1.48906, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-07-1.4891-bigger.hdf5\n",
            "Epoch 8/15\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.4846\n",
            "\n",
            "Epoch 00008: loss improved from 1.48906 to 1.48457, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-08-1.4846-bigger.hdf5\n",
            "Epoch 9/15\n",
            "76916/76916 [==============================] - 164s 2ms/step - loss: 1.4788\n",
            "\n",
            "Epoch 00009: loss improved from 1.48457 to 1.47880, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-09-1.4788-bigger.hdf5\n",
            "Epoch 10/15\n",
            "76916/76916 [==============================] - 163s 2ms/step - loss: 1.4701\n",
            "\n",
            "Epoch 00010: loss improved from 1.47880 to 1.47012, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-10-1.4701-bigger.hdf5\n",
            "Epoch 11/15\n",
            "76916/76916 [==============================] - 160s 2ms/step - loss: 1.4626\n",
            "\n",
            "Epoch 00011: loss improved from 1.47012 to 1.46259, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-11-1.4626-bigger.hdf5\n",
            "Epoch 12/15\n",
            "76916/76916 [==============================] - 160s 2ms/step - loss: 1.4531\n",
            "\n",
            "Epoch 00012: loss improved from 1.46259 to 1.45305, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-12-1.4531-bigger.hdf5\n",
            "Epoch 13/15\n",
            "76916/76916 [==============================] - 162s 2ms/step - loss: 1.4504\n",
            "\n",
            "Epoch 00013: loss improved from 1.45305 to 1.45041, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-13-1.4504-bigger.hdf5\n",
            "Epoch 14/15\n",
            "76916/76916 [==============================] - 163s 2ms/step - loss: 1.4385\n",
            "\n",
            "Epoch 00014: loss improved from 1.45041 to 1.43850, saving model to gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-14-1.4385-bigger.hdf5\n",
            "Epoch 15/15\n",
            "76916/76916 [==============================] - 163s 2ms/step - loss: 1.4419\n",
            "\n",
            "Epoch 00015: loss did not improve from 1.43850\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3a1581ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol59TDUmd_UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "4e0b9e87-af3f-4fd1-e2ed-8b6afccb72fe"
      },
      "source": [
        "# load the network weights\n",
        "filename = \"gdrive/My Drive/EIP 3/phase 2/A2/weights-improvement-14-1.4385-bigger.hdf5\"\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, epochs=0, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f001188dcd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyb2Xa_veven",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "824J1NeNPTIn",
        "colab_type": "code",
        "outputId": "a5e3e6a4-4ed1-4e3a-b530-430138561361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print \"Seed:\"\n",
        "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
        "# generate characters\n",
        "output=\"\"\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "#   print(x.shape)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  output = output+result\n",
        "  pattern = numpy.append(pattern , index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(str(output))\n",
        "print \"\\nDone.\""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" f what such an extraordinary ways of\n",
            "living would be like but it puzzl \"\n",
            "ing ier anice suiet suomeu tealln separking tometie brd seadiises suiet anice sepper anice suiet suomeu tealln separking tomet teparking oureer anice sepper anice suiet suomeu tealln separking tometie brd seadiises suiet anice sepper anice suiet suomeu tealln separking tomet teparking oureer anice sepper anice suiet suomeu tealln separking tometie brd seadiises suiet anice sepper anice suiet suomeu tealln separking tomet teparking oureer anice sepper anice suiet suomeu tealln separking tometie b\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JAJf5QqfDca",
        "colab_type": "text"
      },
      "source": [
        "## Epochs 50-60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27XREn43fzzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"gdrive/My Drive/EIP 3/phase 2/A2/50+\"+\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO_nel-DPS7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a94b7931-e41e-4d6d-ae08-9dd2386fbf41"
      },
      "source": [
        "model.fit(X, y, epochs=10, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76916/76916 [==============================] - 94s 1ms/step - loss: 1.4084\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.40844, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-01-1.4084-bigger.hdf5\n",
            "Epoch 2/10\n",
            "76916/76916 [==============================] - 93s 1ms/step - loss: 1.4001\n",
            "\n",
            "Epoch 00002: loss improved from 1.40844 to 1.40007, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-02-1.4001-bigger.hdf5\n",
            "Epoch 3/10\n",
            "76916/76916 [==============================] - 93s 1ms/step - loss: 1.3902\n",
            "\n",
            "Epoch 00003: loss improved from 1.40007 to 1.39023, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-03-1.3902-bigger.hdf5\n",
            "Epoch 4/10\n",
            "76916/76916 [==============================] - 93s 1ms/step - loss: 1.3828\n",
            "\n",
            "Epoch 00004: loss improved from 1.39023 to 1.38280, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-04-1.3828-bigger.hdf5\n",
            "Epoch 5/10\n",
            "76916/76916 [==============================] - 93s 1ms/step - loss: 1.3735\n",
            "\n",
            "Epoch 00005: loss improved from 1.38280 to 1.37348, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-05-1.3735-bigger.hdf5\n",
            "Epoch 6/10\n",
            "76916/76916 [==============================] - 92s 1ms/step - loss: 1.3719\n",
            "\n",
            "Epoch 00006: loss improved from 1.37348 to 1.37193, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-06-1.3719-bigger.hdf5\n",
            "Epoch 7/10\n",
            "76916/76916 [==============================] - 92s 1ms/step - loss: 1.3700\n",
            "\n",
            "Epoch 00007: loss improved from 1.37193 to 1.37003, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-07-1.3700-bigger.hdf5\n",
            "Epoch 8/10\n",
            "76916/76916 [==============================] - 91s 1ms/step - loss: 1.3649\n",
            "\n",
            "Epoch 00008: loss improved from 1.37003 to 1.36489, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-08-1.3649-bigger.hdf5\n",
            "Epoch 9/10\n",
            "76916/76916 [==============================] - 91s 1ms/step - loss: 1.3599\n",
            "\n",
            "Epoch 00009: loss improved from 1.36489 to 1.35988, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-09-1.3599-bigger.hdf5\n",
            "Epoch 10/10\n",
            "76916/76916 [==============================] - 92s 1ms/step - loss: 1.3505\n",
            "\n",
            "Epoch 00010: loss improved from 1.35988 to 1.35052, saving model to gdrive/My Drive/EIP 3/phase 2/A2/50+weights-improvement-10-1.3505-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0000347390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BziEKCbYgDxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"gdrive/My Drive/EIP 3/phase 2/A2/60+\"+\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xx44DXmYK2I",
        "colab_type": "text"
      },
      "source": [
        "## Epochs 60-70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbznnOMzjwA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f28e694a-a51a-4a73-9783-273c4db1682e"
      },
      "source": [
        "model.fit(X, y, epochs=10, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76916/76916 [==============================] - 58s 759us/step - loss: 1.3170\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.31697, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-01-1.3170-bigger.hdf5\n",
            "Epoch 2/10\n",
            "76916/76916 [==============================] - 58s 760us/step - loss: 1.3019\n",
            "\n",
            "Epoch 00002: loss improved from 1.31697 to 1.30185, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-02-1.3019-bigger.hdf5\n",
            "Epoch 3/10\n",
            "76916/76916 [==============================] - 58s 760us/step - loss: 1.2996\n",
            "\n",
            "Epoch 00003: loss improved from 1.30185 to 1.29960, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-03-1.2996-bigger.hdf5\n",
            "Epoch 4/10\n",
            "76916/76916 [==============================] - 58s 759us/step - loss: 1.2996\n",
            "\n",
            "Epoch 00004: loss improved from 1.29960 to 1.29957, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-04-1.2996-bigger.hdf5\n",
            "Epoch 5/10\n",
            "76916/76916 [==============================] - 58s 759us/step - loss: 1.2964\n",
            "\n",
            "Epoch 00005: loss improved from 1.29957 to 1.29642, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-05-1.2964-bigger.hdf5\n",
            "Epoch 6/10\n",
            "76916/76916 [==============================] - 58s 758us/step - loss: 1.2906\n",
            "\n",
            "Epoch 00006: loss improved from 1.29642 to 1.29061, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-06-1.2906-bigger.hdf5\n",
            "Epoch 7/10\n",
            "76916/76916 [==============================] - 58s 757us/step - loss: 1.2929\n",
            "\n",
            "Epoch 00007: loss did not improve from 1.29061\n",
            "Epoch 8/10\n",
            "76916/76916 [==============================] - 58s 757us/step - loss: 1.2943\n",
            "\n",
            "Epoch 00008: loss did not improve from 1.29061\n",
            "Epoch 9/10\n",
            "76916/76916 [==============================] - 58s 757us/step - loss: 1.2864\n",
            "\n",
            "Epoch 00009: loss improved from 1.29061 to 1.28644, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-09-1.2864-bigger.hdf5\n",
            "Epoch 10/10\n",
            "76916/76916 [==============================] - 58s 759us/step - loss: 1.2823\n",
            "\n",
            "Epoch 00010: loss improved from 1.28644 to 1.28231, saving model to gdrive/My Drive/EIP 3/phase 2/A2/60+weights-improvement-10-1.2823-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0000353110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W_ecR-fj2Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"gdrive/My Drive/EIP 3/phase 2/A2/70+\"+\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0caxY_mNYNpI",
        "colab_type": "text"
      },
      "source": [
        "## Epochs 70-80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQMt5m1xGS-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82b58824-cd3e-4343-e80a-0a4c54694b83"
      },
      "source": [
        "model.fit(X, y, epochs=10, batch_size=1024, callbacks=callbacks_list)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "76916/76916 [==============================] - 45s 579us/step - loss: 1.2672\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.26722, saving model to gdrive/My Drive/EIP 3/phase 2/A2/70+weights-improvement-01-1.2672-bigger.hdf5\n",
            "Epoch 2/10\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2597\n",
            "\n",
            "Epoch 00002: loss improved from 1.26722 to 1.25968, saving model to gdrive/My Drive/EIP 3/phase 2/A2/70+weights-improvement-02-1.2597-bigger.hdf5\n",
            "Epoch 3/10\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2509\n",
            "\n",
            "Epoch 00003: loss improved from 1.25968 to 1.25095, saving model to gdrive/My Drive/EIP 3/phase 2/A2/70+weights-improvement-03-1.2509-bigger.hdf5\n",
            "Epoch 4/10\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2486\n",
            "\n",
            "Epoch 00004: loss improved from 1.25095 to 1.24858, saving model to gdrive/My Drive/EIP 3/phase 2/A2/70+weights-improvement-04-1.2486-bigger.hdf5\n",
            "Epoch 5/10\n",
            "76916/76916 [==============================] - 44s 578us/step - loss: 1.2500\n",
            "\n",
            "Epoch 00005: loss did not improve from 1.24858\n",
            "Epoch 6/10\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2450\n",
            "\n",
            "Epoch 00006: loss improved from 1.24858 to 1.24503, saving model to gdrive/My Drive/EIP 3/phase 2/A2/70+weights-improvement-06-1.2450-bigger.hdf5\n",
            "Epoch 7/10\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2441\n",
            "\n",
            "Epoch 00007: loss improved from 1.24503 to 1.24407, saving model to gdrive/My Drive/EIP 3/phase 2/A2/70+weights-improvement-07-1.2441-bigger.hdf5\n",
            "Epoch 8/10\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2427\n",
            "\n",
            "Epoch 00008: loss improved from 1.24407 to 1.24271, saving model to gdrive/My Drive/EIP 3/phase 2/A2/70+weights-improvement-08-1.2427-bigger.hdf5\n",
            "Epoch 9/10\n",
            "76916/76916 [==============================] - 44s 578us/step - loss: 1.2517\n",
            "\n",
            "Epoch 00009: loss did not improve from 1.24271\n",
            "Epoch 10/10\n",
            "76916/76916 [==============================] - 44s 576us/step - loss: 1.2441\n",
            "\n",
            "Epoch 00010: loss did not improve from 1.24271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7effc60d83d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG5L2P5RYRsZ",
        "colab_type": "text"
      },
      "source": [
        "## Epochs 80-100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A21v8NyxGXaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7b81dc2-1ec7-431f-c84d-50c44f56d348"
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"gdrive/My Drive/EIP 3/phase 2/A2/80+\"+\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "model.fit(X, y, epochs=20, batch_size=1024, callbacks=callbacks_list)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "76916/76916 [==============================] - 45s 579us/step - loss: 1.2370\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.23697, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-01-1.2370-bigger.hdf5\n",
            "Epoch 2/20\n",
            "76916/76916 [==============================] - 44s 578us/step - loss: 1.2360\n",
            "\n",
            "Epoch 00002: loss improved from 1.23697 to 1.23595, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-02-1.2360-bigger.hdf5\n",
            "Epoch 3/20\n",
            "76916/76916 [==============================] - 44s 576us/step - loss: 1.2374\n",
            "\n",
            "Epoch 00003: loss did not improve from 1.23595\n",
            "Epoch 4/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2355\n",
            "\n",
            "Epoch 00004: loss improved from 1.23595 to 1.23550, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-04-1.2355-bigger.hdf5\n",
            "Epoch 5/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2357\n",
            "\n",
            "Epoch 00005: loss did not improve from 1.23550\n",
            "Epoch 6/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2366\n",
            "\n",
            "Epoch 00006: loss did not improve from 1.23550\n",
            "Epoch 7/20\n",
            "76916/76916 [==============================] - 44s 576us/step - loss: 1.2274\n",
            "\n",
            "Epoch 00007: loss improved from 1.23550 to 1.22740, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-07-1.2274-bigger.hdf5\n",
            "Epoch 8/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2287\n",
            "\n",
            "Epoch 00008: loss did not improve from 1.22740\n",
            "Epoch 9/20\n",
            "76916/76916 [==============================] - 44s 578us/step - loss: 1.2362\n",
            "\n",
            "Epoch 00009: loss did not improve from 1.22740\n",
            "Epoch 10/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2299\n",
            "\n",
            "Epoch 00010: loss did not improve from 1.22740\n",
            "Epoch 11/20\n",
            "76916/76916 [==============================] - 44s 576us/step - loss: 1.2203\n",
            "\n",
            "Epoch 00011: loss improved from 1.22740 to 1.22026, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-11-1.2203-bigger.hdf5\n",
            "Epoch 12/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2170\n",
            "\n",
            "Epoch 00012: loss improved from 1.22026 to 1.21702, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-12-1.2170-bigger.hdf5\n",
            "Epoch 13/20\n",
            "76916/76916 [==============================] - 44s 578us/step - loss: 1.2250\n",
            "\n",
            "Epoch 00013: loss did not improve from 1.21702\n",
            "Epoch 14/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2207\n",
            "\n",
            "Epoch 00014: loss did not improve from 1.21702\n",
            "Epoch 15/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2178\n",
            "\n",
            "Epoch 00015: loss did not improve from 1.21702\n",
            "Epoch 16/20\n",
            "76916/76916 [==============================] - 44s 577us/step - loss: 1.2106\n",
            "\n",
            "Epoch 00016: loss improved from 1.21702 to 1.21056, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-16-1.2106-bigger.hdf5\n",
            "Epoch 17/20\n",
            "76916/76916 [==============================] - 44s 575us/step - loss: 1.2100\n",
            "\n",
            "Epoch 00017: loss improved from 1.21056 to 1.21002, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-17-1.2100-bigger.hdf5\n",
            "Epoch 18/20\n",
            "76916/76916 [==============================] - 44s 575us/step - loss: 1.2114\n",
            "\n",
            "Epoch 00018: loss did not improve from 1.21002\n",
            "Epoch 19/20\n",
            "76916/76916 [==============================] - 44s 576us/step - loss: 1.2035\n",
            "\n",
            "Epoch 00019: loss improved from 1.21002 to 1.20348, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-19-1.2035-bigger.hdf5\n",
            "Epoch 20/20\n",
            "76916/76916 [==============================] - 44s 575us/step - loss: 1.2006\n",
            "\n",
            "Epoch 00020: loss improved from 1.20348 to 1.20059, saving model to gdrive/My Drive/EIP 3/phase 2/A2/80+weights-improvement-20-1.2006-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7effc617fdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn_3ZI20YV_-",
        "colab_type": "text"
      },
      "source": [
        "## Final Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gZkaNH0QUb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "6f6c8a4d-aedd-471e-d3f1-0802243d5714"
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print \"Seed:\"\n",
        "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
        "# generate characters\n",
        "output=\"\"\n",
        "for i in range(500):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "#   print(x.shape)\n",
        "  prediction = model.predict(x, verbose=0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = int_to_char[index]\n",
        "  seq_in = [int_to_char[value] for value in pattern]\n",
        "  output = output+result\n",
        "  pattern = numpy.append(pattern , index)\n",
        "  pattern = pattern[1:len(pattern)]\n",
        "print(str(output))\n",
        "print \"\\nDone.\""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" s\n",
            "\n",
            "im sure those are not the right words said poor alice and her eyes\n",
            "\"\n",
            "s whe suesteusily anice hellly whats bhter a little bettualent tane whe catee iardenabte atolet oinutes toatied ier whe coove suiet loow anice hellly whats bhter a little bettualent tane whe catee iardenabte atolet oinutes toatied ier whe coove suiet loow anice hellly whats bhter a little bettualent tane whe catee iardenabte atolet oinutes toatied ier whe coove suiet loow anice hellly whats bhter a little bettualent tane whe catee iardenabte atolet oinutes toatied ier whe coove suiet loow anice \n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzp4ZxWrUIuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}